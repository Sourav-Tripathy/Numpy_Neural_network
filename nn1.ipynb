{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the loss function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred) / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation_func=sigmoid, activation_func_derivative=sigmoid_derivative):\n",
    "        self.layers = layers\n",
    "        self.activation = activation_func\n",
    "        self.activation_derivative = activation_func_derivative\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            # Randomly initialize weights and biases\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i+1]))\n",
    "            self.biases.append(np.random.randn(1, layers[i+1]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.activations = [X]  # Store activations layer by layer\n",
    "        self.z_values = []      # z= aw+b \n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Compute linear combination: z = aW + b\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "\n",
    "            # Apply activation function\n",
    "            a = self.activation(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        return self.activations[-1]  #for next layers z calculations\n",
    "\n",
    "    def backward(self, X, y, loss_func_derivative, learning_rate):\n",
    "\n",
    "        # Calculate output error (loss gradient with respect to predictions)\n",
    "        loss_gradient = loss_func_derivative(y, self.activations[-1])\n",
    "\n",
    "        # Backpropagate through layers here delta is propagating from one layer to another\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Compute gradient for current layer\n",
    "            delta = loss_gradient * self.activation_derivative(self.activations[i+1])\n",
    "\n",
    "            # Gradient for weights and biases\n",
    "            weight_gradient = np.dot(self.activations[i].T, delta)\n",
    "            bias_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    " \n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * weight_gradient\n",
    "            self.biases[i] -= learning_rate * bias_gradient\n",
    "\n",
    "            # Propagate error to previous layer\n",
    "            loss_gradient = np.dot(delta, self.weights[i].T) #this gradient gets reused...and gets updated after one loop\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate, loss_function, loss_derivative):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            # Compute loss\n",
    "            loss = loss_function(y, y_pred)\n",
    "            # Backward pass\n",
    "            self.backward(X, y, loss_derivative, learning_rate)\n",
    "\n",
    "            # Print loss every 1000 epochs\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3859\n",
      "Epoch 1000, Loss: 0.2443\n",
      "Epoch 2000, Loss: 0.2309\n",
      "Epoch 3000, Loss: 0.2098\n",
      "Epoch 4000, Loss: 0.1908\n",
      "Epoch 5000, Loss: 0.1715\n",
      "Epoch 6000, Loss: 0.1377\n",
      "Epoch 7000, Loss: 0.0764\n",
      "Epoch 8000, Loss: 0.0364\n",
      "Epoch 9000, Loss: 0.0206\n",
      "Epoch 10000, Loss: 0.0136\n",
      "Epoch 11000, Loss: 0.0100\n",
      "Epoch 12000, Loss: 0.0077\n",
      "Epoch 13000, Loss: 0.0063\n",
      "Epoch 14000, Loss: 0.0053\n",
      "Epoch 15000, Loss: 0.0045\n",
      "Epoch 16000, Loss: 0.0039\n",
      "Epoch 17000, Loss: 0.0035\n",
      "Epoch 18000, Loss: 0.0031\n",
      "Epoch 19000, Loss: 0.0028\n",
      "Epoch 20000, Loss: 0.0026\n",
      "Epoch 21000, Loss: 0.0024\n",
      "[[0.95497491]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "nn = NeuralNetwork(layers=[2, 2, 1])\n",
    "nn.train(X, y, epochs=22000, learning_rate=0.1, \n",
    "         loss_function=mean_squared_error, \n",
    "         loss_derivative=mean_squared_error_derivative)\n",
    "M=[0,1]\n",
    "output = nn.forward(M)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3412\n",
      "Epoch 1000, Loss: 0.0380\n",
      "Epoch 2000, Loss: 0.0053\n",
      "Epoch 3000, Loss: 0.0020\n",
      "Epoch 4000, Loss: 0.0015\n",
      "Epoch 5000, Loss: 0.0013\n",
      "Epoch 6000, Loss: 0.0012\n",
      "Epoch 7000, Loss: 0.0012\n",
      "Epoch 8000, Loss: 0.0011\n",
      "Epoch 9000, Loss: 0.0011\n",
      "Epoch 10000, Loss: 0.0011\n",
      "Epoch 11000, Loss: 0.0010\n",
      "Epoch 12000, Loss: 0.0010\n",
      "Epoch 13000, Loss: 0.0010\n",
      "Epoch 14000, Loss: 0.0010\n",
      "Epoch 15000, Loss: 0.0010\n",
      "Epoch 16000, Loss: 0.0010\n",
      "Epoch 17000, Loss: 0.0009\n",
      "Epoch 18000, Loss: 0.0009\n",
      "Epoch 19000, Loss: 0.0009\n",
      "Epoch 20000, Loss: 0.0009\n",
      "Epoch 21000, Loss: 0.0009\n",
      "Epoch 22000, Loss: 0.0009\n",
      "Epoch 23000, Loss: 0.0009\n",
      "Epoch 24000, Loss: 0.0009\n",
      "Epoch 25000, Loss: 0.0009\n",
      "Epoch 26000, Loss: 0.0009\n",
      "Epoch 27000, Loss: 0.0009\n",
      "Epoch 28000, Loss: 0.0009\n",
      "Epoch 29000, Loss: 0.0009\n",
      "Epoch 30000, Loss: 0.0008\n",
      "Epoch 31000, Loss: 0.0008\n",
      "Epoch 32000, Loss: 0.0008\n",
      "Epoch 33000, Loss: 0.0008\n",
      "Epoch 34000, Loss: 0.0008\n",
      "Epoch 35000, Loss: 0.0008\n",
      "Epoch 36000, Loss: 0.0008\n",
      "Epoch 37000, Loss: 0.0008\n",
      "Epoch 38000, Loss: 0.0008\n",
      "Epoch 39000, Loss: 0.0008\n",
      "Epoch 40000, Loss: 0.0008\n",
      "Epoch 41000, Loss: 0.0008\n",
      "Epoch 42000, Loss: 0.0008\n",
      "Epoch 43000, Loss: 0.0008\n",
      "Epoch 44000, Loss: 0.0008\n",
      "Epoch 45000, Loss: 0.0008\n",
      "Epoch 46000, Loss: 0.0008\n",
      "Epoch 47000, Loss: 0.0008\n",
      "Epoch 48000, Loss: 0.0008\n",
      "Epoch 49000, Loss: 0.0008\n",
      "Epoch 50000, Loss: 0.0007\n",
      "Epoch 51000, Loss: 0.0007\n",
      "Epoch 52000, Loss: 0.0007\n",
      "Epoch 53000, Loss: 0.0007\n",
      "Epoch 54000, Loss: 0.0007\n",
      "Epoch 55000, Loss: 0.0007\n",
      "Epoch 56000, Loss: 0.0007\n",
      "Epoch 57000, Loss: 0.0007\n",
      "Epoch 58000, Loss: 0.0007\n",
      "Epoch 59000, Loss: 0.0007\n",
      "Epoch 60000, Loss: 0.0007\n",
      "Epoch 61000, Loss: 0.0007\n",
      "Epoch 62000, Loss: 0.0007\n",
      "Epoch 63000, Loss: 0.0007\n",
      "Epoch 64000, Loss: 0.0007\n",
      "Epoch 65000, Loss: 0.0007\n",
      "Epoch 66000, Loss: 0.0007\n",
      "Epoch 67000, Loss: 0.0007\n",
      "Epoch 68000, Loss: 0.0007\n",
      "Epoch 69000, Loss: 0.0007\n",
      "Epoch 70000, Loss: 0.0006\n",
      "Epoch 71000, Loss: 0.0006\n",
      "Epoch 72000, Loss: 0.0006\n",
      "Epoch 73000, Loss: 0.0006\n",
      "Epoch 74000, Loss: 0.0006\n",
      "Epoch 75000, Loss: 0.0006\n",
      "Epoch 76000, Loss: 0.0006\n",
      "Epoch 77000, Loss: 0.0006\n",
      "Epoch 78000, Loss: 0.0006\n",
      "Epoch 79000, Loss: 0.0006\n",
      "Epoch 80000, Loss: 0.0006\n",
      "Epoch 81000, Loss: 0.0006\n",
      "Epoch 82000, Loss: 0.0006\n",
      "Epoch 83000, Loss: 0.0006\n",
      "Epoch 84000, Loss: 0.0006\n",
      "Epoch 85000, Loss: 0.0006\n",
      "Epoch 86000, Loss: 0.0006\n",
      "Epoch 87000, Loss: 0.0006\n",
      "Epoch 88000, Loss: 0.0005\n",
      "Epoch 89000, Loss: 0.0005\n",
      "Epoch 90000, Loss: 0.0005\n",
      "Epoch 91000, Loss: 0.0005\n",
      "Epoch 92000, Loss: 0.0005\n",
      "Epoch 93000, Loss: 0.0005\n",
      "Epoch 94000, Loss: 0.0005\n",
      "Epoch 95000, Loss: 0.0005\n",
      "Epoch 96000, Loss: 0.0005\n",
      "Epoch 97000, Loss: 0.0005\n",
      "Epoch 98000, Loss: 0.0005\n",
      "Epoch 99000, Loss: 0.0005\n",
      "Epoch 100000, Loss: 0.0005\n",
      "Epoch 101000, Loss: 0.0005\n",
      "Epoch 102000, Loss: 0.0005\n",
      "Epoch 103000, Loss: 0.0005\n",
      "Epoch 104000, Loss: 0.0005\n",
      "Epoch 105000, Loss: 0.0005\n",
      "Epoch 106000, Loss: 0.0005\n",
      "Epoch 107000, Loss: 0.0004\n",
      "Epoch 108000, Loss: 0.0004\n",
      "Epoch 109000, Loss: 0.0004\n",
      "Epoch 110000, Loss: 0.0004\n",
      "Epoch 111000, Loss: 0.0004\n",
      "Epoch 112000, Loss: 0.0004\n",
      "Epoch 113000, Loss: 0.0004\n",
      "Epoch 114000, Loss: 0.0004\n",
      "Epoch 115000, Loss: 0.0004\n",
      "Epoch 116000, Loss: 0.0004\n",
      "Epoch 117000, Loss: 0.0004\n",
      "Epoch 118000, Loss: 0.0004\n",
      "Epoch 119000, Loss: 0.0004\n",
      "Epoch 120000, Loss: 0.0004\n",
      "Epoch 121000, Loss: 0.0004\n",
      "Epoch 122000, Loss: 0.0004\n",
      "Epoch 123000, Loss: 0.0004\n",
      "Epoch 124000, Loss: 0.0004\n",
      "Epoch 125000, Loss: 0.0004\n",
      "Epoch 126000, Loss: 0.0004\n",
      "Epoch 127000, Loss: 0.0004\n",
      "Epoch 128000, Loss: 0.0004\n",
      "Epoch 129000, Loss: 0.0003\n",
      "Epoch 130000, Loss: 0.0003\n",
      "Epoch 131000, Loss: 0.0003\n",
      "Epoch 132000, Loss: 0.0003\n",
      "Epoch 133000, Loss: 0.0003\n",
      "Epoch 134000, Loss: 0.0003\n",
      "Epoch 135000, Loss: 0.0003\n",
      "Epoch 136000, Loss: 0.0003\n",
      "Epoch 137000, Loss: 0.0003\n",
      "Epoch 138000, Loss: 0.0003\n",
      "Epoch 139000, Loss: 0.0003\n",
      "Epoch 140000, Loss: 0.0003\n",
      "Epoch 141000, Loss: 0.0003\n",
      "Epoch 142000, Loss: 0.0003\n",
      "Epoch 143000, Loss: 0.0003\n",
      "Epoch 144000, Loss: 0.0003\n",
      "Epoch 145000, Loss: 0.0003\n",
      "Epoch 146000, Loss: 0.0003\n",
      "Epoch 147000, Loss: 0.0003\n",
      "Epoch 148000, Loss: 0.0003\n",
      "Epoch 149000, Loss: 0.0003\n",
      "Epoch 150000, Loss: 0.0003\n",
      "Epoch 151000, Loss: 0.0003\n",
      "Epoch 152000, Loss: 0.0003\n",
      "Epoch 153000, Loss: 0.0003\n",
      "Epoch 154000, Loss: 0.0003\n",
      "Epoch 155000, Loss: 0.0003\n",
      "Epoch 156000, Loss: 0.0003\n",
      "Epoch 157000, Loss: 0.0003\n",
      "Epoch 158000, Loss: 0.0003\n",
      "Epoch 159000, Loss: 0.0003\n",
      "Epoch 160000, Loss: 0.0002\n",
      "Epoch 161000, Loss: 0.0002\n",
      "Epoch 162000, Loss: 0.0002\n",
      "Epoch 163000, Loss: 0.0002\n",
      "Epoch 164000, Loss: 0.0002\n",
      "Epoch 165000, Loss: 0.0002\n",
      "Epoch 166000, Loss: 0.0002\n",
      "Epoch 167000, Loss: 0.0002\n",
      "Epoch 168000, Loss: 0.0002\n",
      "Epoch 169000, Loss: 0.0002\n",
      "Epoch 170000, Loss: 0.0002\n",
      "Epoch 171000, Loss: 0.0002\n",
      "Epoch 172000, Loss: 0.0002\n",
      "Epoch 173000, Loss: 0.0002\n",
      "Epoch 174000, Loss: 0.0002\n",
      "Epoch 175000, Loss: 0.0002\n",
      "Epoch 176000, Loss: 0.0002\n",
      "Epoch 177000, Loss: 0.0002\n",
      "Epoch 178000, Loss: 0.0002\n",
      "Epoch 179000, Loss: 0.0002\n",
      "Epoch 180000, Loss: 0.0002\n",
      "Epoch 181000, Loss: 0.0002\n",
      "Epoch 182000, Loss: 0.0002\n",
      "Epoch 183000, Loss: 0.0002\n",
      "Epoch 184000, Loss: 0.0002\n",
      "Epoch 185000, Loss: 0.0002\n",
      "Epoch 186000, Loss: 0.0002\n",
      "Epoch 187000, Loss: 0.0002\n",
      "Epoch 188000, Loss: 0.0002\n",
      "Epoch 189000, Loss: 0.0002\n",
      "Epoch 190000, Loss: 0.0002\n",
      "Epoch 191000, Loss: 0.0002\n",
      "Epoch 192000, Loss: 0.0002\n",
      "Epoch 193000, Loss: 0.0002\n",
      "Epoch 194000, Loss: 0.0002\n",
      "Epoch 195000, Loss: 0.0002\n",
      "Epoch 196000, Loss: 0.0002\n",
      "Epoch 197000, Loss: 0.0002\n",
      "Epoch 198000, Loss: 0.0002\n",
      "Epoch 199000, Loss: 0.0002\n",
      "Epoch 200000, Loss: 0.0002\n",
      "Epoch 201000, Loss: 0.0002\n",
      "Epoch 202000, Loss: 0.0002\n",
      "Epoch 203000, Loss: 0.0002\n",
      "Epoch 204000, Loss: 0.0002\n",
      "Epoch 205000, Loss: 0.0002\n",
      "Epoch 206000, Loss: 0.0002\n",
      "Epoch 207000, Loss: 0.0002\n",
      "Epoch 208000, Loss: 0.0002\n",
      "Epoch 209000, Loss: 0.0002\n",
      "Epoch 210000, Loss: 0.0002\n",
      "Epoch 211000, Loss: 0.0002\n",
      "Epoch 212000, Loss: 0.0002\n",
      "Epoch 213000, Loss: 0.0002\n",
      "Epoch 214000, Loss: 0.0002\n",
      "Epoch 215000, Loss: 0.0002\n",
      "Epoch 216000, Loss: 0.0002\n",
      "Epoch 217000, Loss: 0.0002\n",
      "Epoch 218000, Loss: 0.0002\n",
      "Epoch 219000, Loss: 0.0002\n",
      "[[0.24894706]]\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(size=1000):\n",
    "    x = np.random.rand(size)\n",
    "    y = x * x\n",
    "    return x.reshape(-1, 1), y.reshape(-1, 1)\n",
    "x, y = generate_dataset()\n",
    "# print(\"First 5 x values:\", x_data[:5])\n",
    "# print(\"First 5 y values:\", y_data[:5])\n",
    "nn = NeuralNetwork(layers=[1, 2, 1])\n",
    "nn.train(x, y, epochs=220000, learning_rate=0.25, \n",
    "         loss_function=mean_squared_error, \n",
    "         loss_derivative=mean_squared_error_derivative)\n",
    "M = np.array([[0.5]])\n",
    "output = nn.forward(M)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
